services:
  snip-agent:
    tty: true
    stdin_open: true
    image: k33g/snip:0.0.5

    ports:
      - 3500:8080

    environment:
      # SYSTEM_INSTRUCTION: |
      #   You are Snip, an AI assistant that helps users to find information.

      # TOOLS_MODEL_INSTRUCTION: |
      #   You have access to the following tools:
      #   - snip_rag_question: 
      #     Use this tool to answer questions based on the knowledge base,
      #     only when the user query message starts with "rag question:".
      SYSTEM_INSTRUCTION: |
        You are Snip, an AI assistant that helps users write and understand code snippets.
        You provide concise explanations, code examples, and best practices for various programming tasks.
        Always aim to be clear, helpful, and accurate in your responses.
        IMPORTANT: When providing code examples, concentrate carefully on syntax accuracy:
        - Close all string quotes properly (" or ')
        - Add semicolons (;) where needed
        - Balance all brackets/parentheses
        - Complete all statements
        - Verify syntax is valid
        Provide only well-formed, executable examples. Double-check before responding.

      TOOLS_MODEL_INSTRUCTION: |
        Use the available tools to assist users with their coding queries.
        When a user asks a question, determine if any tools can help provide a better answer.

      #  Use the snip_rag_question tool ONLY when the user query message starts with "rag question:".


      HTTP_PORT: 8080

      SNIPPETS_FOLDER: snippets
      SIMILARITY_THRESHOLD: 0.5
      SIMILARITY_MAX_RESULTS: 5

      CONTEXT_SIZE_LIMIT: 8192

      MCP_SERVER_BASE_URL: http://mcp-gateway:9011


    volumes:
      - ./data/snippets:/app/snippets

    models:
      snip-model:
        endpoint_var: MODEL_RUNNER_BASE_URL
        model_var: SNIP_MODEL
      embedding-model:
        endpoint_var: MODEL_RUNNER_BASE_URL
        model_var: EMBEDDING_MODEL
      tools-model:
        endpoint_var: MODEL_RUNNER_BASE_URL
        model_var: TOOLS_MODEL

    depends_on:
      mcp-gateway:
        condition: service_started
        
  mcp-gateway:
    # mcp-gateway secures your MCP servers
    image: docker/mcp-gateway:latest
    ports:
      - 9011:9011
    use_api_socket: true
    command:
      - --port=9011
      - --transport=streaming
      - --verbose
      - --catalog=/mcp/catalog.yaml
      - --servers=mcp-docs

    configs:
      - source: catalog.yaml
        target:
          /mcp/catalog.yaml
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      mcp-docs:
        condition: service_healthy

  mcp-docs:
    image: k33g/mcp-docs-server:0.0.1
    # ports:
    #   - 9095:6060
    environment:
      - MCP_HTTP_PORT=6060
      - LIMIT=0.5
      - MAX_RESULTS=5
      - JSON_STORE_FILE_PATH=store/rag-memory-store.json
      - DOCUMENTS_PATH=markdown
      - CHUNK_SIZE=1024
      - CHUNK_OVERLAP=256  
    volumes:
      - ./markdown:/app/markdown
      - ./store/rag:/app/store
    models:
      mcp-rag-model:
        endpoint_var: MODEL_RUNNER_BASE_URL
        model_var: EMBEDDING_MODEL
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:6060/health"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 240s


configs:
  catalog.yaml:
    content: |
      registry:

        mcp-docs:
          remote:
            url: "http://mcp-docs:6060/mcp"
            transport_type: http


models:
  snip-model:
    #model: hf.co/menlo/jan-nano-gguf:q4_k_m
    model: hf.co/menlo/lucy-gguf:q4_k_m
    #model: hf.co/qwen/qwen2.5-coder-1.5b-instruct-gguf:q4_k_m
    #model: hf.co/qwen/qwen2.5-coder-3b-instruct-gguf:q4_k_m
    #model: ai/qwen2.5:latest
    context_size: 8192
    runtime_flags:
        - "--temp"                # Temperature
        - "0.0"                   # Set temperature to 0.0  
        - "--verbose"
        - "--verbose-prompt"

  tools-model:
    model: hf.co/menlo/jan-nano-gguf:q4_k_m
    context_size: 8192
    runtime_flags:
        - "--temp"                # Temperature
        - "0.0"                   # Set temperature to 0.0  
        - "--verbose"
        - "--verbose-prompt"

  embedding-model:
    model: ai/mxbai-embed-large:latest
    #model: ai/nomic-embed-text-v1.5:latest

  mcp-rag-model:
    model: ai/granite-embedding-multilingual:latest